---
layout: post
title:  "Establishing bona fides (aka brags)"
date:   2026-02-12 20:15:18 -0800
---
Like the [About]({{ '/about/' | relative_url }}) page says, I'm aiming to write (and react to writings) about AI, language/linguistics, cognition, and maybe some philosophical stuff. In the (likely futile) hope of showing that I'm not completely out of my depth in doing so, I thought perhaps a page talking about some of my relevant background and experience would help establish some bona fides. I'm pretty sure this will all wind up sounding braggish and self-absorbed instead. So be it. (if you think you're able to ID me by what I've written here then there's a reasonable likelihood that we know each other...go ahead and hit me up at fauxnemic@gmail.com but I ask you to please keep the identity to yourself publicly)

I have a pair of undergraduate degrees, in linguistics and computer science. If you mix & match my electives and whatnot across those degrees I also have enough for a minor in math and am a few courses shy of a degree in philosophy. My graduate degrees (MA & PhD) are in cognitive science, with a focus on linguistics (both formal/pencil & paper, and computational modeling). I also did a year of postdoctoral work looking at information theoretic approaches to some aspects of language. 

I've been reading about and coding neural network simulations of language phenomena since the turn of the century. My first NN was a little Hopfield net that encoded grids corresponding to letters of the (English) alphabet as its stable states/point attractors (you could start it in a state corresponding to a letter corrupted with noise and it would "clean it up" en route to its attractor). For my final projet in my graduate machine learning class I coded a fully recurrent network and its backprop-through-time training loop in C in an attempt at modeling some phonological data (it didn't learn particularly well, but we've learned a lot about data regimes and stabilizing training in the past couple of decades).

After the postdoc I wound up doing "data science" for about 7 or 8 years, mainly for an aging search (i.e. ads) company. We did standard ML stuff like text classification, search result relevance ranking, etc, often just using logistic regression, but also some traditional NLP like parsing & POS tagging, as well as some ngram language modeling. After seeing them presented at PyData 2012 in NYC I advocated for the adoption of word embeddings into some of our classifiers. More recently I'm working at a tech company with a strong AI team, we do speech recognition with biggish neural models, language modeling (my main area, both ngram and neural) and some other things. Our engineers use AI assistants extensively, and our directors are adamant that we stay abreast of developments in this area.

During my time in industry I've made sure to keep a toe in the academic waters, co-organizing workshops that intersect with my areas of focus, and consistently reviewing for ACL conferences and workshops (usually for the Industry Track, but not exclusively), and occasionally submitting short papers or posters to workshops that are in my wheelhouse.

Outside of all this, I'm deeply interested in the questions of (a) what makes a mind (like many people who wound up in CogSci around the time I did, Douglas Hofstadter's GEB was a deeply influential book), (b) what makes a scientific theory count as a good explanation, and (c) are widely differing theoretical accounts of some phenomenon necessarily irreconcilable (this derives from the various approaches to linguistic data I've dabbled in over the years). In all this I find myself resonating most strongly with Angela Potochnik's work, but am still exploring antecedents and other approaches.
