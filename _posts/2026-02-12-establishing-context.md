---
layout: post
title:  "Establishing context"
date:   2026-02-12 20:15:18 -0800
---

I've been studying and thinking about human language and linguistics for over 30 years. I've been reading about and coding neural network simulations of linguistic phenomena since the turn of the century. My first NN was a little Hopfield net that encoded grids corresponding to letters of the (English) alphabet as its stable states/point attractors (you could start it in a state corresponding to a letter corrupted with noise and it would "clean it up" en route to its attractor). For my final projet in my graduate machine learning class I coded a fully recurrent network and its backprop-through-time training loop in C in an attempt at modeling some phonological data (it didn't learn particularly well, but we've learned a lot about data regimes and stabilizing training in the past couple of decades).

After grad school and a postdoc I wound up doing "data science" for 7 or 8 years, mainly for an aging search (i.e. ads) company. We did standard ML stuff like text classification, search result relevance ranking, etc, often just using logistic regression, but also some traditional NLP like parsing & POS tagging, as well as some ngram language modeling. I successfully advocated for the adoption of word embeddings into some of our classifiers and other models after seeing them presented at PyData 2012 in NYC. For the past 6+ years I've been working at a tech company with a strong AI team. We do speech recognition with biggish neural models, language modeling (my main area, both ngram and neural) and NLPish things like sentiment analysis and entity detection, although with LLM training on a considerable internal data set. Our engineers use AI assistants extensively, and our directors are adamant that we stay abreast of developments in this area.

During my time in industry I've made sure to keep a toe in academic waters, co-organizing workshops that intersect with my areas of focus, and consistently reviewing for ACL conferences and workshops (usually for the Industry Track, but not exclusively), and occasionally submitting short papers or posters to workshops that are in my wheelhouse (roughly, computational phonology).

Outside of all this, I'm deeply interested in the questions of (a) what makes a mind (like many people who wound up in CogSci around the time I did, Douglas Hofstadter's GEB was a deeply influential book), (b) what makes a scientific theory count as a good explanation, and (c) are widely differing theoretical accounts of some phenomenon necessarily irreconcilable (this derives from the various approaches to linguistic data I've dabbled in over the years). In all this I find myself resonating most strongly with Angela Potochnik's work, but am still exploring antecedents and other approaches.

